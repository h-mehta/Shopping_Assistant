# -*- coding: utf-8 -*-
"""Demo_version_1.ipynb

Automatically generated by Colab.

"""

from google.colab import drive
drive.mount('/content/drive')
assets_dir = '/content/drive/MyDrive/_datasets/'

"""**Reading the listings**"""

import os
import gzip
import pandas as pd
import numpy as np
import ast

!ls drive/MyDrive/_datasets/Unzipped/abo-listings/listings/metadata/

product_metadata_dir = "drive/MyDrive/_datasets/Unzipped/abo-listings/listings/metadata/"

# List all .json.gz files in the directory
json_files = [f for f in os.listdir(product_metadata_dir) if f.endswith('.json.gz')]

# Create an empty list to store dataframes
dfs = []

# Read each gzip-compressed JSON file and append to the list
for file in json_files:
    file_path = os.path.join(product_metadata_dir, file)
    df = pd.read_json(file_path, compression='gzip', lines=True)
    dfs.append(df)

# Concatenate all dataframes into a single dataframe
df_product = pd.concat(dfs, ignore_index=True)

print("Shape:", df_product.shape)
display(df_product.columns)
display(df_product.head(5))

!ls drive/MyDrive/_datasets/Unzipped/abo-images-small/images/metadata/

# Image listings metadata
images_metadata_path = "drive/MyDrive/_datasets/Unzipped/abo-images-small/images/metadata/images.csv.gz"

df_images = pd.read_csv(images_metadata_path, compression='gzip')

print("Shape:", df_images.shape)
print(df_images.head())

df_merged = df_product.merge(
    df_images,
    left_on="main_image_id",
    right_on="image_id",
    how="left"
)

display(df_merged.head(5))

# Drop products without a valid image path
df_merged = df_merged.dropna(subset=['path'])
# Full image path
base_img_dir = "drive/MyDrive/_datasets/Unzipped/abo-images-small/images/small/"
df_merged['full_path'] = df_merged['path'].apply(lambda x: os.path.join(base_img_dir, x))

from PIL import Image
import os

# Get one product's image path
sample = df_merged.iloc[0]
img_path = os.path.join(base_img_dir, sample["path"])

print(f"Item Name: {sample['item_name'][0]['value']}")
print(f"Brand: {sample['brand'][0]['value']}")
print(f"Description: {sample['product_description']}")
print(f"Image path: {img_path}")
print(f"Bullet points: {sample['bullet_point']}")
print(f"Product type: {sample['product_type'][0]['value']}")
print(f"Item Keywords: {sample['item_keywords'][0]['value']}")
print(f"Pattern: {sample['pattern']}")
print(f"Style: {sample['style']}")
print(f"Color: {sample['color']}")
print(f"Material: {sample['material']}")
print(f"Fabric Type: {sample['fabric_type']}")

# Open image
img = Image.open(img_path)
img.show()

df_merged.columns

df_merged = df_merged.fillna('')

display(df_merged[["item_name","brand","bullet_point"]].head(20))

import pandas as pd

def combine_text(row):
    #print(f"Item Name: {row.get('item_name',[{'value':'NA'}])[0]['value']} ; Brand: {row.get('brand',[{'value':'NA'}])[0]['value']} ; Bullet point : {row.get('bullet_point', [{'value':'NA'}])[0]['value']}")
    #return
    fields = [
        row.get("item_name", ""),
        row.get("brand", ""),
        #row.get("product_type", ""),
        row.get("bullet_point", "")
        #row.get("product_description", "pd"),
        #row.get("item_keywords", ""),
        #row.get("material", ""),
        #row.get("color", ""),
        #row.get("pattern", ""),
        #row.get("style", ""),
        #row.get("fabric_type", "")
    ]
    combined = []
    col_names = ["item_name","brand","bullet_point"]
    for f, column_name in zip(fields, col_names):
        #print(f)
        if isinstance(f, list):
            # Convert list of dictionaries to a string representation
            str_f = str(f[0]['value'])
            if pd.notna(str_f) and str_f.strip() != "" and str_f.strip() != "[]":
                 combined.append(column_name + " : "+str_f)
        elif pd.notna(f):
             combined.append(column_name + " : "+str(f))

    return ";".join(combined)

df_merged["combined_text"] = df_merged.apply(combine_text, axis=1)

df_merged["combined_text"].iloc[1]

# Making the embedding
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

texts = df_merged["combined_text"].tolist()
embeddings = model.encode(texts, batch_size=64, show_progress_bar=True)

df_merged["embeddings"] = embeddings.tolist()

!ls /content/drive/MyDrive/Colab_Notebooks/

# OR Parquet format (faster & smaller)
df_merged.to_csv("/content/drive/MyDrive/Colab_Notebooks/Gen_AI/Intermediate_Dataframes/ShopTalk_df_merged.csv", index=False)

display(df_merged.head(5))

"""# Reading df_merged - to store the embeddings in RAG"""

!pip install transformers datasets torch faiss-cpu wget

!pip install matplotlib scikit-learn

df_merged = pd.read_csv("/content/drive/MyDrive/Colab_Notebooks/Gen_AI/Intermediate_Dataframes/ShopTalk_df_merged.csv")

display(df_merged.head(5))

import faiss

# Convert the string embeddings back to real arrays
df_merged["embeddings"] = df_merged["embeddings"].apply(ast.literal_eval)
embeddings = np.array(df_merged["embeddings"].to_list(), dtype=np.float32)

print(embeddings.shape)

# Normalize embeddings for cosine similarity
faiss.normalize_L2(embeddings)

# Get embedding dimension (should be 384 for MiniLM)
dimension = embeddings.shape[1]

# Create FAISS index for cosine similarity
index = faiss.IndexFlatIP(dimension)

# Add embeddings to FAISS index
index.add(embeddings)

print("Total vectors in FAISS index:", index.ntotal)

# Making the embedding
from sentence_transformers import SentenceTransformer

model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

query = "red leather shoes"  # Example query
query_embedding = model.encode([query])
faiss.normalize_L2(query_embedding)

# Search top 5 closest items
D, I = index.search(query_embedding, k=5)

print("Distances:", D)
print("Indices:", I)
print(df_merged.iloc[I[0]])  # Retrieve top matches

# Checking the result - closest neighbour
df_merged.iloc[63908]